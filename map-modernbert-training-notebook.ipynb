{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":104383,"databundleVersionId":12957508,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### ModernBERT Reference  \nWarner et al., “Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder…”, arXiv:2412.13663 (2024).\n\n@misc{modernbert,\n  title={Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for Fast, Memory Efficient, and Long Context Finetuning and Inference},\n  author={Benjamin Warner and Antoine Chaffin and Benjamin Clavié and Orion Weller and Oskar Hallström and Said Taghadouini and Alexis Gallagher and Raja Biswas and Faisal Ladhak and Tom Aarsen and Nathan Cooper and Griffin Adams and Jeremy Howard and Iacopo Poli},\n  year={2024},\n  eprint={2412.13663},\n  archivePrefix={arXiv},\n  primaryClass={cs.CL},\n  url={https://arxiv.org/abs/2412.13663 }\n}","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2025-08-03T18:29:29.759400Z","iopub.execute_input":"2025-08-03T18:29:29.759691Z","iopub.status.idle":"2025-08-03T18:29:33.632553Z","shell.execute_reply.started":"2025-08-03T18:29:29.759653Z","shell.execute_reply":"2025-08-03T18:29:33.631553Z"}}},{"cell_type":"code","source":"# imports\nimport os, datetime, warnings, re, unicodedata, pandas as pd\nfrom sklearn.model_selection import StratifiedKFold\nfrom tqdm.auto import tqdm\nimport random, numpy as np, torch\nrandom.seed(42); np.random.seed(42); torch.manual_seed(42)\n\nfrom torch import nn, optim\nfrom torch.utils.data import DataLoader\nfrom transformers import (\n    AutoTokenizer,\n    AutoModel,\n    get_linear_schedule_with_warmup,\n    DataCollatorWithPadding\n)\nfrom torch.utils.tensorboard import SummaryWriter  \n\nlogdir = f'tb_logs/run_{datetime.datetime.now():%Y%m%d-%H%M%S}'\nos.makedirs(logdir, exist_ok=True)\nwriter = SummaryWriter(logdir)\nprint(f'TensorBoard logs → {logdir}')\n\nos.environ['TOKENIZERS_PARALLELISM'] = 'false'\nwarnings.filterwarnings('ignore', message=r'Some known HF warning regex')\n\ndf = pd.read_csv('/kaggle/input/map-charting-student-math-misunderstandings/train.csv')\npd.set_option('display.max_colwidth', None)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('Running on:', device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-17T20:13:17.780369Z","iopub.execute_input":"2025-08-17T20:13:17.780645Z","iopub.status.idle":"2025-08-17T20:13:50.070283Z","shell.execute_reply.started":"2025-08-17T20:13:17.780625Z","shell.execute_reply":"2025-08-17T20:13:50.069630Z"}},"outputs":[{"name":"stderr","text":"2025-08-17 20:13:33.301857: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1755461613.678793      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1755461613.783263      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"TensorBoard logs → tb_logs/run_20250817-201349\nRunning on: cuda\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# 0. helper to normalise whitespace/Unicode\ndef _clean(txt: str) -> str:\n    txt = unicodedata.normalize('NFKC', txt)\n    txt = re.sub(r'\\s+', ' ', txt)\n    return txt.strip()\n\n# 1. minimal preprocessing\ndef preprocess(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n\n    # text fields\n    df['StudentExplanation'] = (\n        df['StudentExplanation'].fillna('').apply(_clean)\n    )\n    df['QuestionText'] = df['QuestionText'].apply(_clean)\n    df['MC_Answer']    = df['MC_Answer'].apply(_clean)\n\n    # misconception field\n    df['Misconception'] = (\n        df['Misconception']\n          .fillna('NA')\n          .astype(str)\n          .str.strip()\n          .replace({'Wrong_fraction': 'Wrong_Fraction'})\n    )\n    mask = df['Category'].str.endswith('Misconception')\n    df.loc[~mask, 'Misconception'] = 'NA'\n\n    # joint label string\n    df['label_str'] = df['Category'] + ':' + df['Misconception']\n    return df\n\n# 2. build label maps + attach label_id\ndef build_label_maps(df: pd.DataFrame):\n    labels = sorted(df['label_str'].unique())\n    label2id = {lbl: i for i, lbl in enumerate(labels)}\n    id2label = {i: lbl for lbl, i in label2id.items()}\n    df['label_id'] = df['label_str'].map(label2id).astype(int)\n    return df, label2id, id2label\n\n# 3. run the pipeline\ndf = preprocess(df)                     \ndf, label2id, id2label = build_label_maps(df)   \n\n# 4. stratified K-fold\nwarnings.filterwarnings('ignore', message='The least populated class')\n\nk = 5\nskf = StratifiedKFold(\n    n_splits=k,\n    shuffle=True,\n    random_state=42\n)\n\ndf['fold'] = -1\nfor fold, (_, val_idx) in enumerate(skf.split(df, y=df['label_id'])):\n    df.loc[val_idx, 'fold'] = fold\n\n# sanity check\nassert (df['fold'] >= 0).all()\nprint(df['fold'].value_counts().sort_index())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-17T20:13:50.071549Z","iopub.execute_input":"2025-08-17T20:13:50.071789Z","iopub.status.idle":"2025-08-17T20:13:50.734267Z","shell.execute_reply.started":"2025-08-17T20:13:50.071769Z","shell.execute_reply":"2025-08-17T20:13:50.733574Z"}},"outputs":[{"name":"stdout","text":"fold\n0    7340\n1    7339\n2    7339\n3    7339\n4    7339\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# 5. loading ModernBERT tokenizer\nMODEL_NAME = 'answerdotai/ModernBERT-base'\nNUM_LABELS = len(label2id)\nMAX_LEN = 256\n\n# 6. fast tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\n    MODEL_NAME,\n    use_fast=True\n)\nSPECIAL_TOKENS = ['<Q>', '</Q>', '<A>', '</A>', '<E>', '</E>']\ntokenizer.add_special_tokens({'additional_special_tokens': SPECIAL_TOKENS})\nfor tok in SPECIAL_TOKENS:\n    assert tokenizer.convert_tokens_to_ids(tok) != tokenizer.unk_token_id, f'{tok} is UNK!'\n    \n# 7. template builder\nTEMPLATE = '<Q> {q} </Q> <A> {a} </A> <E> {e} </E>'\n\ndef build_text(row):\n    return TEMPLATE.format(\n        q=row['QuestionText'],\n        a=row['MC_Answer'],\n        e=row['StudentExplanation']\n    )\n\ndf['text'] = df.apply(build_text, axis=1)\n\n# 8. sampling sequence lenght distribution\ntok_lens = df['text'].apply(lambda s: len(tokenizer.tokenize(s)))\nprint(tok_lens.describe(percentiles=[.5,.75,.9,.95,.99]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-17T20:13:50.734958Z","iopub.execute_input":"2025-08-17T20:13:50.735194Z","iopub.status.idle":"2025-08-17T20:13:58.918723Z","shell.execute_reply.started":"2025-08-17T20:13:50.735173Z","shell.execute_reply":"2025-08-17T20:13:58.917779Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3b4e84a8a2944f3add58d5645aae657"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc5f093d4147458081a26e1d1585128e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/694 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0acf8bf361304de096f1a82861b4ca9c"}},"metadata":{}},{"name":"stdout","text":"count    36696.000000\nmean        63.591699\nstd         18.033133\nmin         26.000000\n50%         62.000000\n75%         74.000000\n90%         87.000000\n95%         98.000000\n99%        114.000000\nmax        222.000000\nName: text, dtype: float64\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# 9. PyTorch Dataset  ─ no token_type_ids for ModernBERT\nclass MAPDataset(torch.utils.data.Dataset):\n    def __init__(self, frame, tokenizer):\n        self.encodings = tokenizer(\n            frame['text'].tolist(),\n            truncation=True,\n            max_length=MAX_LEN,\n            return_attention_mask=True,\n            return_token_type_ids=False,\n        )\n        self.labels = frame['label_id'].tolist()\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        item = {k: v[idx] for k, v in self.encodings.items()}  \n        item['labels'] = self.labels[idx]                      \n        return item\n\n# 10. dataframe → DataLoader\ncollate_fn = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8)\n\ndef make_loader(frame, batch_size=16, shuffle=True, num_workers=2):\n    ds = MAPDataset(frame, tokenizer)\n\n    kwargs = dict(\n        dataset=ds,\n        batch_size=batch_size,\n        shuffle=shuffle,\n        collate_fn=collate_fn,\n        pin_memory=True,\n        num_workers=num_workers,\n    )\n    if num_workers > 0:\n        kwargs.update(dict(persistent_workers=True, prefetch_factor=2))\n\n    return DataLoader(**kwargs)\n\n\n'''\n# 11. hold-out fold\nval_fold = 4\n\ntrain_df = df[df['fold'] != val_fold].reset_index(drop=True)\nval_df = df[df['fold'] == val_fold].reset_index(drop=True)\nprint(len(train_df), 'train rows |', len(val_df), 'val rows')\n\ntrain_loader = make_loader(train_df, batch_size=16, shuffle=True,  num_workers=2)\nval_loader = make_loader(val_df, batch_size=32, shuffle=False, num_workers=2)\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-17T20:13:58.920506Z","iopub.execute_input":"2025-08-17T20:13:58.920885Z","iopub.status.idle":"2025-08-17T20:13:58.930477Z","shell.execute_reply.started":"2025-08-17T20:13:58.920865Z","shell.execute_reply":"2025-08-17T20:13:58.929670Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"\"\\n# 11. hold-out fold\\nval_fold = 4\\n\\ntrain_df = df[df['fold'] != val_fold].reset_index(drop=True)\\nval_df = df[df['fold'] == val_fold].reset_index(drop=True)\\nprint(len(train_df), 'train rows |', len(val_df), 'val rows')\\n\\ntrain_loader = make_loader(train_df, batch_size=16, shuffle=True,  num_workers=2)\\nval_loader = make_loader(val_df, batch_size=32, shuffle=False, num_workers=2)\\n\""},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"MODEL_NAME = 'answerdotai/ModernBERT-base'\nNUM_LABELS = len(label2id)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# 12. ModernBERT + single-layer classification head (no token_type_ids)\nclass ModernBertClassifier(nn.Module):\n    def __init__(self, num_labels: int = NUM_LABELS, dropout: float = 0.1):\n        super().__init__()\n        self.encoder = AutoModel.from_pretrained(MODEL_NAME)\n        self.encoder.resize_token_embeddings(len(tokenizer))\n        self.dropout = nn.Dropout(dropout)\n        self.classifier = nn.Linear(self.encoder.config.hidden_size, num_labels)\n\n    def forward(self, input_ids, attention_mask=None, labels=None):\n        out = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n        cls_vec = out.last_hidden_state[:, 0]                     \n        logits = self.classifier(self.dropout(cls_vec))\n\n        loss = None\n        if labels is not None:\n            loss = nn.functional.cross_entropy(logits, labels)\n\n        return {'logits': logits, 'loss': loss}\n\n# 13. factory → model, optimizer, scheduler\ndef build_model(total_train_steps, lr=2e-5, weight_decay=0.01, warmup_ratio=0.1):\n    model = ModernBertClassifier().to(device)\n\n    # weight-decay only on non-bias / non-LayerNorm parameters\n    no_decay = {'bias', 'LayerNorm.weight'}\n    param_groups = [\n        {\n            'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n            'weight_decay': weight_decay,\n        },\n        {\n            'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n            'weight_decay': 0.0,\n        },\n    ]\n\n    optimizer = optim.AdamW(param_groups, lr=lr)\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=int(total_train_steps * warmup_ratio),\n        num_training_steps=total_train_steps,\n    )\n    return model, optimizer, scheduler","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-17T20:13:58.931299Z","iopub.execute_input":"2025-08-17T20:13:58.931567Z","iopub.status.idle":"2025-08-17T20:13:58.965257Z","shell.execute_reply.started":"2025-08-17T20:13:58.931547Z","shell.execute_reply":"2025-08-17T20:13:58.964497Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# 14. training the model + val metrics MAP@3 — AMP version\n'''\n%load_ext tensorboard\n%tensorboard --logdir tb_logs --host 0.0.0.0\n\nepochs = 10\ntotal_steps = len(train_loader) * epochs\nmodel, optim, sched = build_model(total_train_steps=total_steps)\nscaler = torch.cuda.amp.GradScaler()                     \n\nfor epoch in range(epochs):\n    model.train()\n    running = 0.0\n\n    for batch in tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}', leave=False):\n        batch = {k: v.to(device) for k, v in batch.items()}\n\n        with torch.cuda.amp.autocast():                  \n            out  = model(**batch)\n            loss = out['loss']\n\n        scaler.scale(loss).backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  \n        scaler.step(optim)\n        scaler.update()\n        sched.step()\n        optim.zero_grad(set_to_none=True)\n\n        running += loss.item()\n\n    avg_train = running / len(train_loader)\n    writer.add_scalar('Loss/train', avg_train, epoch)\n\n    # validation\n    model.eval()\n    val_running, preds, y_true = 0.0, [], []\n    with torch.no_grad(), torch.cuda.amp.autocast():\n        for batch in val_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            out = model(**batch)\n\n            val_running += out['loss'].item()\n            preds.append(out['logits'].float().cpu())    \n            y_true.extend(batch['labels'].cpu())\n\n    avg_val = val_running / len(val_loader)\n    writer.add_scalar('Loss/val', avg_val, epoch)\n\n    # MAP@3\n    preds = torch.vstack(preds)                          \n    top3 = torch.topk(preds, 3, dim=1).indices.cpu()    \n    hits = (top3 == torch.tensor(y_true).unsqueeze(1))  \n    ranks = hits.float() * (1 / (torch.arange(1, 4).float()))  \n    map3 = ranks.sum(dim=1).mean().item()\n\n    writer.add_scalar('MAP3/val', map3, epoch)\n    print(f'Epoch {epoch+1}: train_loss={avg_train:.4f}  '\n          f'val_loss={avg_val:.4f}  MAP@3={map3:.4f}')\n'''\n\n# --- helpers ---\ndef predict_logits(model, loader):\n    model.eval()\n    outs = []\n    with torch.no_grad(), torch.amp.autocast('cuda'):\n        for batch in loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            outs.append(model(**batch)[\"logits\"].float().cpu())\n    return torch.vstack(outs)  # (N, NUM_LABELS)\n\ndef map3_from_logits(logits, y_true):\n    top3 = torch.topk(logits, 3, dim=1).indices.cpu()        # (N, 3)\n    y_true = torch.as_tensor(y_true).unsqueeze(1)            # (N, 1)\n    hits = (top3 == y_true)                                  # boolean\n    ranks = hits.float() * (1.0 / torch.arange(1, 4).float())# [1, 1/2, 1/3]\n    return ranks.sum(dim=1).mean().item()\n\n# --- 5-fold training + OOF collection with best-on-val checkpointing ---\nepochs = 10\npatience = 2  # early stopping after 'patience' epochs without MAP@3 improvement\nfold_scores = []\noof_logits = torch.zeros((len(df), NUM_LABELS), dtype=torch.float32)\ncheckpoints = []\n\nfor val_fold in range(5):\n    # precise split that preserves original indices for OOF placement\n    val_idx = df.index[df[\"fold\"] == val_fold].to_numpy()\n    val_df  = df.loc[val_idx].reset_index(drop=True)\n    train_df = df[df[\"fold\"] != val_fold].reset_index(drop=True)\n\n    train_loader = make_loader(train_df, batch_size=16, shuffle=True)\n    val_loader = make_loader(val_df, batch_size=32, shuffle=False)\n\n    # fresh model/opt/sched/scaler\n    total_steps = len(train_loader) * epochs\n    model, optimizer, sched = build_model(total_train_steps=total_steps)\n    scaler = torch.amp.GradScaler('cuda')\n\n    best_map3 = -1.0\n    best_val_logits = None\n    best_path = f\"/kaggle/working/modernbert_fold{val_fold}_best.pt\"\n    no_improve = 0\n\n    # ----- train -----\n    for epoch in range(epochs):\n        model.train()\n        running = 0.0\n\n        for batch in tqdm(train_loader, desc=f\"F{val_fold} E{epoch+1}/{epochs}\", leave=False):\n            batch = {k: v.to(device) for k, v in batch.items()}\n            with torch.amp.autocast('cuda'):\n                out = model(**batch)\n                loss = out[\"loss\"]\n\n            scaler.scale(loss).backward()\n            scaler.unscale_(optimizer)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            scaler.step(optimizer)\n            scaler.update()\n            sched.step()\n            optimizer.zero_grad(set_to_none=True)\n\n            running += loss.item()\n\n        # ----- end-of-epoch validation -----\n        val_logits = predict_logits(model, val_loader)\n        y_true = val_df[\"label_id\"].tolist()\n        cur_map3 = map3_from_logits(val_logits, y_true)\n        print(f\"Fold {val_fold} Epoch {epoch+1}: train_loss={running/len(train_loader):.4f}  MAP@3={cur_map3:.4f}\")\n\n        # checkpoint the best\n        if cur_map3 > best_map3:\n            best_map3 = cur_map3\n            best_val_logits = val_logits\n            torch.save(model.state_dict(), best_path)\n            no_improve = 0\n        else:\n            no_improve += 1\n            if no_improve >= patience:\n                print(f\"Early stopping on fold {val_fold} at epoch {epoch+1}\")\n                break\n\n    # OOF using the best epoch's logits, aligned to original df indices (all Torch)\n    val_idx_t = torch.as_tensor(val_idx, dtype=torch.long)\n    if best_val_logits is None:\n        best_val_logits = predict_logits(model, val_loader)\n        y_true = val_df[\"label_id\"].tolist()\n        best_map3 = map3_from_logits(best_val_logits, y_true)\n        torch.save(model.state_dict(), best_path)  \n        \n    oof_logits.index_copy_(0, val_idx_t, best_val_logits)        \n    fold_scores.append(best_map3)\n    checkpoints.append(best_path)\n    \n    # ---- free some RAM/VRAM before next fold ----\n    del train_loader, val_loader        \n    del model, optimizer, sched         \n    import gc; gc.collect()            \n    torch.cuda.empty_cache()            \n\nprint(\"CV MAP@3 (mean over folds):\", sum(fold_scores)/len(fold_scores))\nprint(\"Best checkpoints:\", checkpoints)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-17T20:13:58.966197Z","iopub.execute_input":"2025-08-17T20:13:58.966445Z","execution_failed":"2025-08-17T21:08:32.613Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4865e796428a42278c389a8d13f3642e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/599M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4ca9603707a40ccaf852fbef8ca4ea9"}},"metadata":{}},{"name":"stderr","text":"The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"F0 E1/10:   0%|          | 0/1835 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Fold 0 Epoch 1: train_loss=1.3544  MAP@3=0.8297\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"F0 E2/10:   0%|          | 0/1835 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Fold 0 Epoch 2: train_loss=0.6400  MAP@3=0.8869\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"F0 E3/10:   0%|          | 0/1835 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Fold 0 Epoch 3: train_loss=0.5030  MAP@3=0.9047\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"F0 E4/10:   0%|          | 0/1835 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Fold 0 Epoch 4: train_loss=0.4150  MAP@3=0.9139\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"F0 E5/10:   0%|          | 0/1835 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"170ca1c0883646beab809af677418c5f"}},"metadata":{}}],"execution_count":null}]}