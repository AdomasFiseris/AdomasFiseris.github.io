{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":104383,"databundleVersionId":12957508,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### ModernBERT Reference  \nWarner et al., “Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder…”, arXiv:2412.13663 (2024).\n\n@misc{modernbert,\n  title={Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for Fast, Memory Efficient, and Long Context Finetuning and Inference},\n  author={Benjamin Warner and Antoine Chaffin and Benjamin Clavié and Orion Weller and Oskar Hallström and Said Taghadouini and Alexis Gallagher and Raja Biswas and Faisal Ladhak and Tom Aarsen and Nathan Cooper and Griffin Adams and Jeremy Howard and Iacopo Poli},\n  year={2024},\n  eprint={2412.13663},\n  archivePrefix={arXiv},\n  primaryClass={cs.CL},\n  url={https://arxiv.org/abs/2412.13663 }\n}","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2025-08-03T18:29:29.759400Z","iopub.execute_input":"2025-08-03T18:29:29.759691Z","iopub.status.idle":"2025-08-03T18:29:33.632553Z","shell.execute_reply.started":"2025-08-03T18:29:29.759653Z","shell.execute_reply":"2025-08-03T18:29:33.631553Z"}}},{"cell_type":"code","source":"# imports\n!pip install -q tensorboardX\n\nimport os, datetime, warnings, re, unicodedata, pandas as pd\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom tqdm.auto import tqdm\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nimport torch.optim as optim\nfrom transformers import (\n    AutoTokenizer,\n    AutoModel,\n    default_data_collator,\n    get_linear_schedule_with_warmup,\n)\nfrom tensorboardX import SummaryWriter\n\nlogdir = f\"tb_logs/run_{datetime.datetime.now():%Y%m%d-%H%M%S}\"\nos.makedirs(logdir, exist_ok=True)\nwriter = SummaryWriter(logdir)\nprint(f\"TensorBoard logs → {logdir}\")\n\nfor dirname, _, filenames in os.walk(\"/kaggle/input\"):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\ndf = pd.read_csv(\"/kaggle/input/map-charting-student-math-misunderstandings/train.csv\")\npd.set_option(\"display.max_colwidth\", None)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Running on:\", device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T01:44:05.232720Z","iopub.execute_input":"2025-08-04T01:44:05.233387Z","iopub.status.idle":"2025-08-04T01:44:49.353454Z","shell.execute_reply.started":"2025-08-04T01:44:05.233357Z","shell.execute_reply":"2025-08-04T01:44:49.352691Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"},{"name":"stderr","text":"2025-08-04 01:44:19.909729: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1754271860.140302      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1754271860.205390      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"TensorBoard logs → tb_logs/run_20250804-014449\n/kaggle/input/map-charting-student-math-misunderstandings/sample_submission.csv\n/kaggle/input/map-charting-student-math-misunderstandings/train.csv\n/kaggle/input/map-charting-student-math-misunderstandings/test.csv\nRunning on: cuda\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# 0. helper to normalise whitespace/Unicode\ndef _clean(txt: str) -> str:\n    txt = unicodedata.normalize(\"NFKC\", txt)\n    txt = re.sub(r\"\\s+\", \" \", txt)\n    return txt.strip()\n\n# 1. minimal preprocessing\ndef preprocess(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n\n    # text fields\n    df[\"StudentExplanation\"] = (\n        df[\"StudentExplanation\"].fillna(\"\").apply(_clean)\n    )\n    df[\"QuestionText\"] = df[\"QuestionText\"].apply(_clean)\n    df[\"MC_Answer\"]    = df[\"MC_Answer\"].apply(_clean)\n\n    # misconception field\n    df[\"Misconception\"] = (\n        df[\"Misconception\"]\n          .fillna(\"NA\")\n          .astype(str)\n          .str.strip()\n          .replace({\"Wrong_fraction\": \"Wrong_Fraction\"})\n    )\n    mask = df[\"Category\"].str.endswith(\"Misconception\")\n    df.loc[~mask, \"Misconception\"] = \"NA\"\n\n    # joint label string\n    df[\"label_str\"] = df[\"Category\"] + \":\" + df[\"Misconception\"]\n    return df\n\n# 2. build label maps + attach label_id\ndef build_label_maps(df: pd.DataFrame):\n    labels = sorted(df[\"label_str\"].unique())\n    label2id = {lbl: i for i, lbl in enumerate(labels)}\n    id2label = {i: lbl for lbl, i in label2id.items()}\n    df[\"label_id\"] = df[\"label_str\"].map(label2id).astype(int)\n    return df, label2id, id2label\n\n# 3. run the pipeline\ndf = preprocess(df)                     \ndf, label2id, id2label = build_label_maps(df)   \n\n# 4. stratified group K-fold on QuestionId\nwarnings.filterwarnings(\"ignore\", message=\"The least populated class\")\n\nk = 5\nskf = StratifiedKFold(\n    n_splits=k,\n    shuffle=True,\n    random_state=42\n)\n\ndf[\"fold\"] = -1\nfor fold, (_, val_idx) in enumerate(skf.split(df, y=df[\"label_id\"])):\n    df.loc[val_idx, \"fold\"] = fold\n\n# sanity check\nassert (df[\"fold\"] >= 0).all()\nprint(df[\"fold\"].value_counts().sort_index())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T01:44:56.136653Z","iopub.execute_input":"2025-08-04T01:44:56.137785Z","iopub.status.idle":"2025-08-04T01:44:56.751610Z","shell.execute_reply.started":"2025-08-04T01:44:56.137742Z","shell.execute_reply":"2025-08-04T01:44:56.750781Z"}},"outputs":[{"name":"stdout","text":"fold\n0    7340\n1    7339\n2    7339\n3    7339\n4    7339\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# 5. loading ModernBERT tokenizer\nMODEL_NAME = 'answerdotai/ModernBERT-base'\nNUM_LABELS = len(label2id)\nMAX_LEN = 128\n\n# 6. fast tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\n    MODEL_NAME,\n    use_fast=True\n)\nSPECIAL_TOKENS = [\"<Q>\", \"</Q>\", \"<A>\", \"</A>\", \"<E>\", \"</E>\"]\ntokenizer.add_special_tokens({\"additional_special_tokens\": SPECIAL_TOKENS})\n\n# 7. template builder\nTEMPLATE = \"<Q> {q} </Q> <A> {a} </A> <E> {e} </E>\"\n\ndef build_text(row):\n    return TEMPLATE.format(\n        q=row[\"QuestionText\"],\n        a=row[\"MC_Answer\"],\n        e=row[\"StudentExplanation\"]\n    )\n\ndf[\"text\"] = df.apply(build_text, axis=1)\n\n# 8. sampling sequence lenght distribution\ntok_lens = df[\"text\"].apply(lambda s: len(tokenizer.tokenize(s)))\nprint(tok_lens.describe(percentiles=[.5,.75,.9,.95,.99]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T01:44:59.585045Z","iopub.execute_input":"2025-08-04T01:44:59.585811Z","iopub.status.idle":"2025-08-04T01:45:08.050245Z","shell.execute_reply.started":"2025-08-04T01:44:59.585781Z","shell.execute_reply":"2025-08-04T01:45:08.049410Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ebc7eff72c846e3a207db10ec6f3f4e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"076bac8306b74d62a3eedb42a38c3fb4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/694 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55b6487a1b59416cb8e578fdd8f0174d"}},"metadata":{}},{"name":"stdout","text":"count    36696.000000\nmean        63.591699\nstd         18.033133\nmin         26.000000\n50%         62.000000\n75%         74.000000\n90%         87.000000\n95%         98.000000\n99%        114.000000\nmax        222.000000\nName: text, dtype: float64\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# 9. PyTorch Dataset  ─ no token_type_ids for ModernBERT\nclass MAPDataset(torch.utils.data.Dataset):\n    def __init__(self, frame):\n        self.texts  = frame[\"text\"].tolist()\n        self.labels = frame[\"label_id\"].tolist()\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        enc = tokenizer(\n            self.texts[idx],\n            padding=\"max_length\",\n            truncation=True,\n            max_length=MAX_LEN,\n            return_attention_mask=True,\n            return_token_type_ids=False,\n        )\n        return {\n            \"input_ids\": torch.tensor(enc[\"input_ids\"], dtype=torch.long),\n            \"attention_mask\": torch.tensor(enc[\"attention_mask\"], dtype=torch.long),\n            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long),\n        }\n\n# 10. dataframe → DataLoader\ndef make_loader(frame, batch_size=16, shuffle=True):\n    ds = MAPDataset(frame)\n    return DataLoader(\n        ds,\n        batch_size=batch_size,\n        shuffle=shuffle,\n        num_workers=2,\n        pin_memory=True,\n    )\n\n# 11. hold-out fold\nval_fold = 4\n\ntrain_df = df[df[\"fold\"] != val_fold].reset_index(drop=True)\nval_df = df[df[\"fold\"] == val_fold].reset_index(drop=True)\nprint(len(train_df), \"train rows |\", len(val_df), \"val rows\")\n\ntrain_loader = make_loader(train_df, batch_size=16, shuffle=True)\nval_loader = make_loader(val_df, batch_size=32, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T01:45:09.958559Z","iopub.execute_input":"2025-08-04T01:45:09.958869Z","iopub.status.idle":"2025-08-04T01:45:09.995366Z","shell.execute_reply.started":"2025-08-04T01:45:09.958846Z","shell.execute_reply":"2025-08-04T01:45:09.994702Z"}},"outputs":[{"name":"stdout","text":"29357 train rows | 7339 val rows\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"MODEL_NAME = \"answerdotai/ModernBERT-base\"\nNUM_LABELS = len(label2id)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# 12. ModernBERT + single-layer classification head (no token_type_ids)\nclass ModernBertClassifier(nn.Module):\n    def __init__(self, num_labels: int = NUM_LABELS, dropout: float = 0.1):\n        super().__init__()\n        self.encoder = AutoModel.from_pretrained(MODEL_NAME)\n        self.encoder.resize_token_embeddings(len(tokenizer))\n        self.dropout = nn.Dropout(dropout)\n        self.classifier = nn.Linear(self.encoder.config.hidden_size, num_labels)\n\n    def forward(self, input_ids, attention_mask=None, labels=None):\n        out = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n        cls_vec = out.last_hidden_state[:, 0]                     \n        logits = self.classifier(self.dropout(cls_vec))\n\n        loss = None\n        if labels is not None:\n            loss = nn.functional.cross_entropy(logits, labels)\n\n        return {\"logits\": logits, \"loss\": loss}\n\n# 13. factory → model, optimizer, scheduler\ndef build_model(total_train_steps, lr=2e-5, weight_decay=0.01, warmup_ratio=0.1):\n    model = ModernBertClassifier().to(device)\n\n    # weight-decay only on non-bias / non-LayerNorm parameters\n    no_decay = {\"bias\", \"LayerNorm.weight\"}\n    param_groups = [\n        {\n            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n            \"weight_decay\": weight_decay,\n        },\n        {\n            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n            \"weight_decay\": 0.0,\n        },\n    ]\n\n    optimizer = optim.AdamW(param_groups, lr=lr)\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=int(total_train_steps * warmup_ratio),\n        num_training_steps=total_train_steps,\n    )\n    return model, optimizer, scheduler","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T01:45:13.694899Z","iopub.execute_input":"2025-08-04T01:45:13.695181Z","iopub.status.idle":"2025-08-04T01:45:13.705023Z","shell.execute_reply.started":"2025-08-04T01:45:13.695158Z","shell.execute_reply":"2025-08-04T01:45:13.704213Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# 14. training the model + val metrics MAP@3\n%load_ext tensorboard\n%tensorboard --logdir tb_logs --host 0.0.0.0\n    \nepochs = 10\ntotal_steps = len(train_loader) * epochs\nmodel, optim, sched = build_model(total_train_steps=total_steps)\n\nfor epoch in range(epochs):\n    model.train()\n    running = 0.0\n\n    # tqdm shows batch-level progress for this epoch\n    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\", leave=False):\n        batch = {k: v.to(device) for k, v in batch.items()}\n\n        out = model(**batch)\n        loss = out[\"loss\"]\n        loss.backward()\n\n        optim.step()\n        sched.step()\n        optim.zero_grad()\n\n        running += loss.item()\n\n    avg_train = running / len(train_loader)\n    writer.add_scalar(\"Loss/train\", avg_train, epoch)\n\n    # validation\n    model.eval()\n    val_running, preds, y_true = 0.0, [], []\n    with torch.no_grad():\n        for batch in val_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            out = model(**batch)\n\n            val_running += out[\"loss\"].item()\n            preds.append(out[\"logits\"].cpu().numpy())\n            y_true.extend(batch[\"labels\"].cpu().numpy())\n\n    avg_val = val_running / len(val_loader)\n    writer.add_scalar(\"Loss/val\", avg_val, epoch)\n\n    # quick MAP@3\n    import numpy as np\n    preds = np.vstack(preds)\n    top3  = preds.argsort(axis=1)[:, -3:][:, ::-1]\n    map3  = np.mean([\n        1/(row.tolist().index(y)+1) if y in row else 0\n        for y, row in zip(y_true, top3)\n    ])\n    writer.add_scalar(\"MAP3/val\", map3, epoch)\n\n    print(f\"Epoch {epoch+1}: train_loss={avg_train:.4f}  \"\n          f\"val_loss={avg_val:.4f}  MAP@3={map3:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T01:45:16.576447Z","iopub.execute_input":"2025-08-04T01:45:16.577014Z","iopub.status.idle":"2025-08-04T03:17:20.332807Z","shell.execute_reply.started":"2025-08-04T01:45:16.576987Z","shell.execute_reply":"2025-08-04T03:17:20.331577Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"\n        (async () => {\n            const url = new URL(await google.colab.kernel.proxyPort(6006, {'cache': true}));\n            url.searchParams.set('tensorboardColab', 'true');\n            const iframe = document.createElement('iframe');\n            iframe.src = url;\n            iframe.setAttribute('width', '100%');\n            iframe.setAttribute('height', '800');\n            iframe.setAttribute('frameborder', 0);\n            document.body.appendChild(iframe);\n        })();\n    "},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b7009ab0d3340cc8e8e7fcfddf01e98"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/599M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fcf6b6f9680944ab87a5dc873d94d895"}},"metadata":{}},{"name":"stderr","text":"The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 1/5:   0%|          | 0/1835 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 1: train_loss=1.2030  val_loss=0.6428  MAP@3=0.8653\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 2/5:   0%|          | 0/1835 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 2: train_loss=0.5814  val_loss=0.5311  MAP@3=0.8908\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 3/5:   0%|          | 0/1835 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 3: train_loss=0.4851  val_loss=0.4469  MAP@3=0.9096\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 4/5:   0%|          | 0/1835 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 4: train_loss=0.3750  val_loss=0.4038  MAP@3=0.9229\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 5/5:   0%|          | 0/1835 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 5: train_loss=0.2797  val_loss=0.3854  MAP@3=0.9257\n","output_type":"stream"}],"execution_count":6}]}