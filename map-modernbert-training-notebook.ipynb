{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":104383,"databundleVersionId":12957508,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### ModernBERT Reference  \nWarner et al., “Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder…”, arXiv:2412.13663 (2024).\n\n@misc{modernbert,\n  title={Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for Fast, Memory Efficient, and Long Context Finetuning and Inference},\n  author={Benjamin Warner and Antoine Chaffin and Benjamin Clavié and Orion Weller and Oskar Hallström and Said Taghadouini and Alexis Gallagher and Raja Biswas and Faisal Ladhak and Tom Aarsen and Nathan Cooper and Griffin Adams and Jeremy Howard and Iacopo Poli},\n  year={2024},\n  eprint={2412.13663},\n  archivePrefix={arXiv},\n  primaryClass={cs.CL},\n  url={https://arxiv.org/abs/2412.13663 }\n}","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2025-08-03T18:29:29.759400Z","iopub.execute_input":"2025-08-03T18:29:29.759691Z","iopub.status.idle":"2025-08-03T18:29:33.632553Z","shell.execute_reply.started":"2025-08-03T18:29:29.759653Z","shell.execute_reply":"2025-08-03T18:29:33.631553Z"}}},{"cell_type":"code","source":"# imports\nimport os, datetime, warnings, re, unicodedata, pandas as pd\nfrom sklearn.model_selection import StratifiedKFold\nfrom tqdm.auto import tqdm\nimport random, numpy as np, torch\nrandom.seed(42); np.random.seed(42); torch.manual_seed(42)\n\nfrom torch import nn, optim\nfrom torch.utils.data import DataLoader\nfrom transformers import (\n    AutoTokenizer,\n    AutoModel,\n    get_linear_schedule_with_warmup,\n    DataCollatorWithPadding\n)\nfrom torch.utils.tensorboard import SummaryWriter  \n\nlogdir = f'tb_logs/run_{datetime.datetime.now():%Y%m%d-%H%M%S}'\nos.makedirs(logdir, exist_ok=True)\nwriter = SummaryWriter(logdir)\nprint(f'TensorBoard logs → {logdir}')\n\nos.environ['TOKENIZERS_PARALLELISM'] = 'false'\nwarnings.filterwarnings('ignore', message=r'Some known HF warning regex')\n\ndf = pd.read_csv('/kaggle/input/map-charting-student-math-misunderstandings/train.csv')\npd.set_option('display.max_colwidth', None)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('Running on:', device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T00:45:54.757988Z","iopub.execute_input":"2025-08-05T00:45:54.758263Z","iopub.status.idle":"2025-08-05T00:46:28.957734Z","shell.execute_reply.started":"2025-08-05T00:45:54.758241Z","shell.execute_reply":"2025-08-05T00:46:28.957067Z"}},"outputs":[{"name":"stderr","text":"2025-08-05 00:46:11.376048: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1754354771.766379      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1754354771.872150      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"TensorBoard logs → tb_logs/run_20250805-004628\nRunning on: cuda\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# 0. helper to normalise whitespace/Unicode\ndef _clean(txt: str) -> str:\n    txt = unicodedata.normalize('NFKC', txt)\n    txt = re.sub(r'\\s+', ' ', txt)\n    return txt.strip()\n\n# 1. minimal preprocessing\ndef preprocess(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n\n    # text fields\n    df['StudentExplanation'] = (\n        df['StudentExplanation'].fillna('').apply(_clean)\n    )\n    df['QuestionText'] = df['QuestionText'].apply(_clean)\n    df['MC_Answer']    = df['MC_Answer'].apply(_clean)\n\n    # misconception field\n    df['Misconception'] = (\n        df['Misconception']\n          .fillna('NA')\n          .astype(str)\n          .str.strip()\n          .replace({'Wrong_fraction': 'Wrong_Fraction'})\n    )\n    mask = df['Category'].str.endswith('Misconception')\n    df.loc[~mask, 'Misconception'] = 'NA'\n\n    # joint label string\n    df['label_str'] = df['Category'] + ':' + df['Misconception']\n    return df\n\n# 2. build label maps + attach label_id\ndef build_label_maps(df: pd.DataFrame):\n    labels = sorted(df['label_str'].unique())\n    label2id = {lbl: i for i, lbl in enumerate(labels)}\n    id2label = {i: lbl for lbl, i in label2id.items()}\n    df['label_id'] = df['label_str'].map(label2id).astype(int)\n    return df, label2id, id2label\n\n# 3. run the pipeline\ndf = preprocess(df)                     \ndf, label2id, id2label = build_label_maps(df)   \n\n# 4. stratified K-fold\nwarnings.filterwarnings('ignore', message='The least populated class')\n\nk = 5\nskf = StratifiedKFold(\n    n_splits=k,\n    shuffle=True,\n    random_state=42\n)\n\ndf['fold'] = -1\nfor fold, (_, val_idx) in enumerate(skf.split(df, y=df['label_id'])):\n    df.loc[val_idx, 'fold'] = fold\n\n# sanity check\nassert (df['fold'] >= 0).all()\nprint(df['fold'].value_counts().sort_index())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T00:46:35.935761Z","iopub.execute_input":"2025-08-05T00:46:35.936063Z","iopub.status.idle":"2025-08-05T00:46:36.543378Z","shell.execute_reply.started":"2025-08-05T00:46:35.936040Z","shell.execute_reply":"2025-08-05T00:46:36.542696Z"}},"outputs":[{"name":"stdout","text":"fold\n0    7340\n1    7339\n2    7339\n3    7339\n4    7339\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# 5. loading ModernBERT tokenizer\nMODEL_NAME = 'answerdotai/ModernBERT-base'\nNUM_LABELS = len(label2id)\nMAX_LEN = 256\n\n# 6. fast tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\n    MODEL_NAME,\n    use_fast=True\n)\nSPECIAL_TOKENS = ['<Q>', '</Q>', '<A>', '</A>', '<E>', '</E>']\ntokenizer.add_special_tokens({'additional_special_tokens': SPECIAL_TOKENS})\nfor tok in SPECIAL_TOKENS:\n    assert tokenizer.convert_tokens_to_ids(tok) != tokenizer.unk_token_id, f'{tok} is UNK!'\n    \n# 7. template builder\nTEMPLATE = '<Q> {q} </Q> <A> {a} </A> <E> {e} </E>'\n\ndef build_text(row):\n    return TEMPLATE.format(\n        q=row['QuestionText'],\n        a=row['MC_Answer'],\n        e=row['StudentExplanation']\n    )\n\ndf['text'] = df.apply(build_text, axis=1)\n\n# 8. sampling sequence lenght distribution\ntok_lens = df['text'].apply(lambda s: len(tokenizer.tokenize(s)))\nprint(tok_lens.describe(percentiles=[.5,.75,.9,.95,.99]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T00:46:42.820243Z","iopub.execute_input":"2025-08-05T00:46:42.820882Z","iopub.status.idle":"2025-08-05T00:46:50.932868Z","shell.execute_reply.started":"2025-08-05T00:46:42.820848Z","shell.execute_reply":"2025-08-05T00:46:50.932175Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"328885578a864a03b8add322ddcca435"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be38ee507d4c4378ac56980116c52fdb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/694 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48d2d4b3bb0a48d6bbdbdb06d1c06e5e"}},"metadata":{}},{"name":"stdout","text":"count    36696.000000\nmean        63.591699\nstd         18.033133\nmin         26.000000\n50%         62.000000\n75%         74.000000\n90%         87.000000\n95%         98.000000\n99%        114.000000\nmax        222.000000\nName: text, dtype: float64\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# 9. PyTorch Dataset  ─ no token_type_ids for ModernBERT\nclass MAPDataset(torch.utils.data.Dataset):\n    def __init__(self, frame, tokenizer):\n        self.encodings = tokenizer(\n            frame['text'].tolist(),\n            truncation=True,\n            max_length=MAX_LEN,\n            return_attention_mask=True,\n            return_token_type_ids=False,\n        )\n        self.labels = frame['label_id'].tolist()\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        item = {k: v[idx] for k, v in self.encodings.items()}  \n        item['labels'] = self.labels[idx]                      \n        return item\n\n# 10. dataframe → DataLoader\ncollate_fn = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8)\n\ndef make_loader(frame, batch_size=16, shuffle=True):\n    ds = MAPDataset(frame, tokenizer)\n    return DataLoader(\n        ds,\n        batch_size=batch_size,\n        shuffle=shuffle,\n        collate_fn=collate_fn,\n        num_workers=2,\n        pin_memory=True,\n    )\n\n# 11. hold-out fold\nval_fold = 4\n\ntrain_df = df[df['fold'] != val_fold].reset_index(drop=True)\nval_df = df[df['fold'] == val_fold].reset_index(drop=True)\nprint(len(train_df), 'train rows |', len(val_df), 'val rows')\n\ntrain_loader = make_loader(train_df, batch_size=16, shuffle=True)\nval_loader = make_loader(val_df, batch_size=32, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T00:46:57.762136Z","iopub.execute_input":"2025-08-05T00:46:57.762700Z","iopub.status.idle":"2025-08-05T00:47:04.047103Z","shell.execute_reply.started":"2025-08-05T00:46:57.762672Z","shell.execute_reply":"2025-08-05T00:47:04.046381Z"}},"outputs":[{"name":"stdout","text":"29357 train rows | 7339 val rows\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"MODEL_NAME = 'answerdotai/ModernBERT-base'\nNUM_LABELS = len(label2id)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# 12. ModernBERT + single-layer classification head (no token_type_ids)\nclass ModernBertClassifier(nn.Module):\n    def __init__(self, num_labels: int = NUM_LABELS, dropout: float = 0.1):\n        super().__init__()\n        self.encoder = AutoModel.from_pretrained(MODEL_NAME)\n        self.encoder.resize_token_embeddings(len(tokenizer))\n        self.dropout = nn.Dropout(dropout)\n        self.classifier = nn.Linear(self.encoder.config.hidden_size, num_labels)\n\n    def forward(self, input_ids, attention_mask=None, labels=None):\n        out = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n        cls_vec = out.last_hidden_state[:, 0]                     \n        logits = self.classifier(self.dropout(cls_vec))\n\n        loss = None\n        if labels is not None:\n            loss = nn.functional.cross_entropy(logits, labels)\n\n        return {'logits': logits, 'loss': loss}\n\n# 13. factory → model, optimizer, scheduler\ndef build_model(total_train_steps, lr=2e-5, weight_decay=0.01, warmup_ratio=0.1):\n    model = ModernBertClassifier().to(device)\n\n    # weight-decay only on non-bias / non-LayerNorm parameters\n    no_decay = {'bias', 'LayerNorm.weight'}\n    param_groups = [\n        {\n            'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n            'weight_decay': weight_decay,\n        },\n        {\n            'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n            'weight_decay': 0.0,\n        },\n    ]\n\n    optimizer = optim.AdamW(param_groups, lr=lr)\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=int(total_train_steps * warmup_ratio),\n        num_training_steps=total_train_steps,\n    )\n    return model, optimizer, scheduler","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T00:47:09.094387Z","iopub.execute_input":"2025-08-05T00:47:09.094708Z","iopub.status.idle":"2025-08-05T00:47:09.104154Z","shell.execute_reply.started":"2025-08-05T00:47:09.094673Z","shell.execute_reply":"2025-08-05T00:47:09.103314Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# 14. training the model + val metrics MAP@3 — AMP version\n%load_ext tensorboard\n%tensorboard --logdir tb_logs --host 0.0.0.0\n\nepochs = 10\ntotal_steps = len(train_loader) * epochs\nmodel, optim, sched = build_model(total_train_steps=total_steps)\nscaler = torch.cuda.amp.GradScaler()                     \n\nfor epoch in range(epochs):\n    model.train()\n    running = 0.0\n\n    for batch in tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}', leave=False):\n        batch = {k: v.to(device) for k, v in batch.items()}\n\n        with torch.cuda.amp.autocast():                  \n            out  = model(**batch)\n            loss = out['loss']\n\n        scaler.scale(loss).backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  \n        scaler.step(optim)\n        scaler.update()\n        sched.step()\n        optim.zero_grad(set_to_none=True)\n\n        running += loss.item()\n\n    avg_train = running / len(train_loader)\n    writer.add_scalar('Loss/train', avg_train, epoch)\n\n    # validation\n    model.eval()\n    val_running, preds, y_true = 0.0, [], []\n    with torch.no_grad(), torch.cuda.amp.autocast():\n        for batch in val_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            out = model(**batch)\n\n            val_running += out['loss'].item()\n            preds.append(out['logits'].float().cpu())    \n            y_true.extend(batch['labels'].cpu())\n\n    avg_val = val_running / len(val_loader)\n    writer.add_scalar('Loss/val', avg_val, epoch)\n\n    # MAP@3\n    preds = torch.vstack(preds)                          \n    top3 = torch.topk(preds, 3, dim=1).indices.cpu()    \n    hits = (top3 == torch.tensor(y_true).unsqueeze(1))  \n    ranks = hits.float() * (1 / (torch.arange(1, 4).float()))  \n    map3 = ranks.sum(dim=1).mean().item()\n\n    writer.add_scalar('MAP3/val', map3, epoch)\n    print(f'Epoch {epoch+1}: train_loss={avg_train:.4f}  '\n          f'val_loss={avg_val:.4f}  MAP@3={map3:.4f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T00:47:15.453083Z","iopub.execute_input":"2025-08-05T00:47:15.453372Z","iopub.status.idle":"2025-08-05T01:45:16.465095Z","shell.execute_reply.started":"2025-08-05T00:47:15.453353Z","shell.execute_reply":"2025-08-05T01:45:16.463455Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"\n        (async () => {\n            const url = new URL(await google.colab.kernel.proxyPort(6006, {'cache': true}));\n            url.searchParams.set('tensorboardColab', 'true');\n            const iframe = document.createElement('iframe');\n            iframe.src = url;\n            iframe.setAttribute('width', '100%');\n            iframe.setAttribute('height', '800');\n            iframe.setAttribute('frameborder', 0);\n            document.body.appendChild(iframe);\n        })();\n    "},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7edb3602ac214297adb20745fa0bbad8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/599M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dbbb8da30aa84a09acfc481a9ca29062"}},"metadata":{}},{"name":"stderr","text":"The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n/tmp/ipykernel_36/1903122820.py:8: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 1/10:   0%|          | 0/1835 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/1903122820.py:17: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n/tmp/ipykernel_36/1903122820.py:36: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.no_grad(), torch.cuda.amp.autocast():\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1: train_loss=1.4822  val_loss=0.7932  MAP@3=0.8328\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 2/10:   0%|          | 0/1835 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 2: train_loss=0.6155  val_loss=0.5464  MAP@3=0.8878\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 3/10:   0%|          | 0/1835 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 3: train_loss=0.4916  val_loss=0.4720  MAP@3=0.9071\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 4/10:   0%|          | 0/1835 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 4: train_loss=0.4162  val_loss=0.4586  MAP@3=0.9119\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 5/10:   0%|          | 0/1835 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 5: train_loss=0.3946  val_loss=0.4448  MAP@3=0.9143\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 6/10:   0%|          | 0/1835 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 6: train_loss=0.3529  val_loss=0.4118  MAP@3=0.9224\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 7/10:   0%|          | 0/1835 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 7: train_loss=0.2616  val_loss=0.4121  MAP@3=0.9251\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 8/10:   0%|          | 0/1835 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 8: train_loss=0.1991  val_loss=0.4206  MAP@3=0.9275\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 9/10:   0%|          | 0/1835 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 9: train_loss=0.1639  val_loss=0.4427  MAP@3=0.9257\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 10/10:   0%|          | 0/1835 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 10: train_loss=0.1323  val_loss=0.4576  MAP@3=0.9280\n","output_type":"stream"}],"execution_count":6}]}