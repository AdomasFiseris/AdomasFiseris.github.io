<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <title>Adomas Fiseris | Data Scientist, AVP at Citizens</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
  <meta name="description" content="Data Scientist focused on second line of defense risk intelligence at a regional bank." />
  <meta name="keywords" content="risk data science, compliance analytics, machine learning, Kaggle, banking, MLOps, streaming inference" />
  <link rel="canonical" href="https://adomasfiseris.github.io/" />

  <!-- Open Graph -->
  <meta property="og:type" content="website" />
  <meta property="og:title" content="Adomas Fiseris | Data Scientist, AVP at Citizens" />
  <meta property="og:description" content="Data Scientist focused on second line of defense risk intelligence at a regional bank." />
  <meta property="og:url" content="https://adomasfiseris.github.io/" />
  <meta property="og:image" content="https://adomasfiseris.github.io/images/YourPhoto.jpg" />
  <meta name="twitter:card" content="summary_large_image" />

  <link rel="stylesheet" href="assets/css/main.css" />
  <noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>

  <!-- Inter font -->
  <link rel="preload" href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600&display=swap" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <noscript><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600&display=swap"></noscript>

  <style>
    body, input, select, textarea {
      font-family: 'Inter', sans-serif !important;
      font-size: 17px;
      line-height: 1.7;
      color: #fafbfd;
      background: #050608;
    }
    h1, h2, h3, h4, h5, h6, .button { text-transform: none !important; color: #fafbfd; }
    h1.major { letter-spacing: 0.02em !important; font-size: 2.3rem; margin-bottom: 0.35rem; text-decoration: none; }
    h2 { letter-spacing: 0.01em !important; font-weight: 600; color: #fafbfd; text-decoration: none; margin-top: 0.2rem; margin-bottom: 0.4rem; }
    h3, h4, h5, h6 { letter-spacing: 0.005em !important; color: #f1f5f9; text-decoration: none; }
    section.project h4 { font-size: 1.08rem; }
    .project h4 { text-decoration: none; }

    #header { display: none !important; }
    #main-wrapper, main, footer { display: block !important; opacity: 1 !important; transform: none !important; }
    body.is-preload, body.is-preload * { animation: none !important; }

    a { color: #9acbff; }
    a:visited { color: #cfd4ff; }
    a:hover, a:focus { text-decoration: underline; color: #eaf2ff; }

    .skip-link {
      position: absolute;
      left: -999px;
      top: 0;
      background: #0f1116;
      color: #f8fafc;
      padding: 0.55rem 1rem;
      z-index: 1000;
      border-radius: 6px;
    }
    .skip-link:focus { left: 0; }

    #wrapper { padding: 0 0 3rem 0; }
    #home { max-width: 1040px; margin: 0 auto; padding: 2.25rem 1.25rem 3rem; }

    section.project { margin-bottom: 2rem; background: #0c0f15; padding: 1.25rem 1.4rem; border-radius: 12px; border: 1px solid #1b2330; box-shadow: 0 12px 28px rgba(0,0,0,0.35); }
    section.project h4 { margin-top: 0; }

    #profile-pic { width: 32%; max-width: 260px; height: auto; border-radius: 12px; box-shadow: 0 12px 28px rgba(0,0,0,0.45); }
    .pill { display: inline-block; padding: 0.32rem 0.7rem; margin: 0 0.35rem 0.35rem 0; border-radius: 999px; background: rgba(255,255,255,0.08); color: #f6f7fb; font-size: 0.95rem; border: 1px solid rgba(255,255,255,0.14); }
    .section-heading { margin: 1.4rem 0 0.35rem; letter-spacing: 0.08em; text-transform: uppercase; font-size: 0.95rem; color: #fafbfd; }
    .section-lead { margin-top: 0; color: #e2e8f0; }

    /* layout helpers for readability */
    .intro-narrow {
      max-width: 40rem;
      margin: 0 0 1.9rem 0;
      text-align: left;
      font-size: 1rem;
      color: #fafbfd;
    }
    .project-body {
      font-size: 0.98rem;
      line-height: 1.75;
      max-width: 46rem;
      margin: 0.35rem 0 0 0;
      color: #f1f5f9;
    }
    .project-body p { margin: 0.45rem 0; }
    .project-body ul { margin: 0.45rem 0 0.2rem 1.15rem; }
    .project-body li { margin: 0.2rem 0; }

    /* collapsible project details */
    details.project-details {
      margin-top: 0.5rem;
      border-top: 1px solid #1b2330;
      padding-top: 0.6rem;
    }
    details.project-details summary {
      cursor: pointer;
      list-style: none;
      color: #e5edff;
      font-weight: 600;
      letter-spacing: 0.01em;
      display: inline-flex;
      align-items: center;
      gap: 0.35rem;
    }
    details.project-details summary::-webkit-details-marker { display: none; }
    details.project-details[open] summary { color: #fafbfd; }
    details.project-details summary .chevron {
      display: inline-block;
      transition: transform 0.2s ease;
      font-size: 0.9rem;
    }
    details.project-details[open] summary .chevron { transform: rotate(90deg); }

    /* align main text with image edge */
    #home > h3,
    #home > p,
    #home > ul,
    #home > section.project,
    #home > .section-lead {
      max-width: 46rem;
      margin-left: 0;
    }
    
    /* education spacing */
    .edu-heading {
      margin-top: 2rem;
    }

    /* top contact shortcuts */
    .contact-top {
      margin: 0.4rem 0 1.4rem 0;
    }
    .contact-top ul.icons {
      margin: 0;
    }
  </style>
</head>
<body class="is-preload">
  <a href="#main-content" class="skip-link">skip to content</a>

  <div id="wrapper">
    <main id="main-content">
      <article id="home">
        <h1 class="major">Adomas Fiseris</h1>
        <h2>Data Scientist, AVP at Citizens</h2>

        <div class="contact-top">
          <ul class="icons">
            <li><a href="https://www.linkedin.com/in/adomas-fiseris-10577b156" class="icon brands fa-linkedin" target="_blank" aria-label="LinkedIn profile of Adomas Fiseris"><span class="label">LinkedIn</span></a></li>
            <li><a href="https://github.com/AdomasFiseris" class="icon brands fa-github" target="_blank" aria-label="GitHub profile of Adomas Fiseris"><span class="label">GitHub</span></a></li>
            <li><a href="mailto:adomasfiseris@gmail.com" class="icon solid fa-envelope" aria-label="Email Adomas Fiseris"><span class="label">Email</span></a></li>
          </ul>
        </div>

        <span class="image main">
          <img id="profile-pic" src="images/YourPhoto.jpg" alt="Portrait of Adomas Fiseris" loading="eager" />
        </span>

        <p class="intro-narrow">I’m a risk data scientist and AVP at Citizens focused on consumer bank compliance and operational risk analytics. I design and orchestrate end-to-end Python and SQL pipelines on a cloud-native analytics platform to turn large-scale transaction data into production-ready risk scores and decision tools. My work blends feature engineering and model development on modern ML frameworks with scalable ETL, rigorous data quality monitoring, and audit-ready validation in partnership with Risk, Compliance, and Engineering teams. I keep my practice sharp through Kaggle competitions, benchmarking new methods and bringing proven ideas back into regulated production environments.</p>

        <h3 class="section-heading edu-heading">Education</h3>
        <p class="section-lead">Current focus: NLP for conduct risk, streaming inference, production observability.</p>
        <section class="project">
          <h4>Georgia Tech</h4>
          <div class="project-body">
            <p><strong>Program:</strong> Master of Science in Analytics, Computational Track - Data Science and ML</p>
            <p><strong>Start:</strong> January 2026 (two-year program)</p>
          </div>
        </section>
        <section class="project">
          <h4>ISM University of Management and Economics</h4>
          <div class="project-body">
            <p><strong>Degree:</strong> Bachelor's degree in Economics</p>
            <p><strong>Years:</strong> 2012 - 2016</p>
            <p><strong>Activities:</strong> University of Economics (VSE) Prague Erasmus exchange, spring semester 2015</p>
            <p><strong>GPA:</strong> 3.7</p>
            <p><strong>Honors:</strong> 100 Talent scholarship</p>
          </div>
        </section>

        <h3 class="section-heading">Kaggle &amp; Trading Competitions</h3>
        <section class="project">
          <h4>Hull Tactical - Market Prediction (Kaggle 2025)</h4>
          <div class="project-body">
            <p><strong>Competition:</strong> Kaggle Hull Tactical: Market Prediction | <strong>Placement:</strong> Pending (competition active) | <strong>Best public score:</strong> 0.494 (Sharpe-like metric)</p>
            <p><strong>Approach:</strong> streaming ensemble of LightGBM and ElasticNet with a volatility aware allocation policy.</p>
            <details class="project-details" open>
              <summary><span class="chevron">▶</span> Full details</summary>
              <p><strong>Overview:</strong> predict daily S&P 500 excess returns and convert them into allocations between 0 and 2 while staying within 120% of index volatility. Kaggle evaluation streams one day at a time via the API to prevent lookahead.</p>
              <p><strong>Feature engineering:</strong></p>
              <ul>
                <li>Explicit feature contract so inference schema matches training for single row streaming.</li>
                <li>Rolling means, standard deviations, and z scores over 21-, 42-, and 63-day windows for key signals like lagged_forward_returns.</li>
                <li>Short lags such as 2-day and 5-day for local momentum and mean reversion.</li>
                <li>Group proxies for macro, rates, and sentiment capturing level, one day delta, five day trend, and spread.</li>
                <li>Expanding mean imputer with stored scaling stats so inputs are fully imputed and standardized exactly as in training.</li>
              </ul>
              <p><strong>Return ensemble:</strong></p>
              <ul>
                <li>Four LightGBM boosters on the engineered set (about 289 features) averaged into a single gradient boosting signal.</li>
                <li>Four ElasticNet regressions serialized as coefficient and intercept bundles, averaged into a linear signal.</li>
                <li>Blend as ensemble_pred = w_lgb * lgb_mean + w_enet * elastic_mean with weights tuned offline.</li>
              </ul>
              <p><strong>Volatility aware allocation:</strong></p>
              <ul>
                <li>SmoothRegimePolicy maps predicted excess return to allocation using a leverage coefficient k_t based on rolling volatility (lagged_forward_returns__vol__w21). Low volatility means higher sensitivity, high volatility de leverages automatically.</li>
                <li>VolGovernor tracks an EWMA of strategy variance versus market variance and scales allocations to target the 1.2x volatility cap.</li>
                <li>Daily loop: compute ensemble_pred, apply SmoothRegimePolicy, adjust with VolGovernor, clip to [0, 2], and return allocation to the API.</li>
              </ul>
              <p><strong>Link:</strong> <a href="https://www.kaggle.com/code/adomaslvu/hull-tactical-market-prediction-6-0?scriptVersionId=282978321" target="_blank">Kaggle submission notebook</a></p>
            </details>
          </div>
        </section>

        <section class="project">
          <h4>MAP - Charting Student Math Misunderstandings (Kaggle 2025)</h4>
          <div class="project-body">
            <p><strong>Competition:</strong> Kaggle MAP: Charting Student Math Misunderstandings | <strong>Placement:</strong> 962/1,857 | <strong>Private score:</strong> 0.94310 MAP@3</p>
            <p><strong>Approach:</strong> deep learning ensemble (DeepSeek 7B, Gemma2-9B, ModernBERT-base) targeting ranked Category:Misconception labels.</p>
            <details class="project-details">
              <summary><span class="chevron">▶</span> Full details</summary>
              <p><strong>Problem:</strong> diagnose math misconceptions from open-ended explanations (ages 9–16). For each response, predict up to three Category:Misconception labels covering correctness, presence of misconception, and specific misconception, evaluated with MAP@3.</p>
              <p><strong>Modeling strategy:</strong></p>
              <ul>
                <li><strong>Unified label space:</strong> canonical Category:Misconception set, normalized label variants, and reindexed outputs into a shared class order before fusion.</li>
                <li><strong>Correctness aware inputs:</strong> derived question level correctness flags and injected them as text prompts and ModernBERT special tokens to guide predictions.</li>
                <li><strong>Ensemble:</strong> ModernBERT-base sequence classifier (64 class focus) with templated question, answer, explanation, plus a correctness token, DeepSeek 7B classifier over the full label set, and Gemma2-9B IT with LoRA adapters and a compact correctness prompt.</li>
                <li><strong>Fusion and decoding:</strong> log probability blends (60% DeepSeek, 35% Gemma2-9B IT, 5% ModernBERT) with top three labels selected from the unified space.</li>
              </ul>
              <p><strong>Link:</strong> <a href="https://www.kaggle.com/code/adomaslvu/map-gemma-mbert-deepseek-submission-2?scriptVersionId=266234776" target="_blank">Kaggle submission notebook</a></p>
            </details>
          </div>
        </section>

        <section class="project">
          <h4>Predict Calorie Expenditure (Kaggle Playground S5E5, 2025)</h4>
          <div class="project-body">
            <p><strong>Competition:</strong> Kaggle Playground Series S5E5: Predict Calorie Expenditure | <strong>Placement:</strong> 1,533 / 4,316 teams | <strong>Best public score:</strong> 0.05794 RMSLE | <strong>Best private score:</strong> 0.05942 RMSLE</p>
            <p><strong>Final model:</strong> stacked ensemble of XGBoost, CatBoost, and a neural network with a Ridge meta learner.</p>
            <details class="project-details">
              <summary><span class="chevron">▶</span> Full details</summary>
              <p><strong>Overview:</strong> tabular regression to predict workout calorie burn on a large synthetic dataset derived from Calories Burnt Prediction. Target is continuous calories, metric is RMSLE, and submissions require id with Calories for each test row.</p>
              <p><strong>Feature engineering:</strong></p>
              <ul>
                <li>BMI computed as Weight divided by (Height divided by 100) squared.</li>
                <li>Timed intensity as Duration multiplied by Heart_Rate.</li>
                <li>Heart rate zone percent of max as Heart_Rate divided by (220 - Age).</li>
                <li>Mifflin-St Jeor BMR as a sex-specific basal metabolic rate estimate.</li>
                <li>One hot encoding for Sex where needed.</li>
              </ul>
              <p><strong>Base models (Optuna tuned, 5-fold CV):</strong></p>
              <ul>
                <li>CatBoost with depth 10, about 1.6k trees, learning rate around 0.083.</li>
                <li>XGBoost with histogram based trees, tuned depth, learning rate, and regularization.</li>
                <li>Neural network with three dense layers (256 to 32 to 64), GELU activations, dropout around 0.17, early stopping via SciKeras.</li>
              </ul>
              <p><strong>Ensemble:</strong> Ridge stack on out of fold predictions from XGBoost, CatBoost, and the neural network to produce the final leaderboard submission.</p>
              <p><strong>Link:</strong> <a href="https://github.com/AdomasFiseris/Kaggle-Playground" target="_blank">GitHub repo</a> (modeling done outside Kaggle, CSV predictions submitted).</p>
            </details>
          </div>
        </section>

        <section class="project">
          <h4>Jane Street Real-Time Market Data Forecasting (Kaggle 2024-2025)</h4>
          <div class="project-body">
            <p><strong>Competition:</strong> Kaggle Jane Street Real-Time Market Data Forecasting | <strong>Placement:</strong> N/A (submission failed in later test phases) | <strong>Best leaderboard score:</strong> 0.006171 weighted R squared on responder_6</p>
            <p><strong>Models:</strong> XGBoost gradient boosted trees and a two layer LSTM evaluated separately, with the XGBoost run producing the top score.</p>
            <details class="project-details">
              <summary><span class="chevron">▶</span> Full details</summary>
              <p><strong>Overview:</strong> real-time responder_6 forecasting on roughly 4.5 million rows per phase with 79 anonymized features and nine responders. Submission is via the streaming evaluation API with strict runtime constraints, reflecting non-stationary, heavy-tailed market data and evolving private tests.</p>
              <p><strong>Data and validation:</strong></p>
              <ul>
                <li>Used 79 raw features plus optional one day lags of all responders from lags.parquet, keeping feature engineering minimal to avoid regime overfitting and to stay within memory and time limits.</li>
                <li>Chronological validation: trained on early periods and held out about the last 100 days as a pseudo live set to mirror leaderboard behavior.</li>
                <li>Implemented the official weighted zero mean R squared metric inside XGBoost and PyTorch loops for early stopping and aligned model selection.</li>
              </ul>
              <p><strong>XGBoost setup:</strong> histogram tree method, tuned with Optuna (max_depth 4, learning_rate about 0.10, subsample about 0.71, colsample_bytree about 0.73, gamma about 0.26), trained around 75 rounds with early stopping; best public and private score was 0.006171.</p>
              <p><strong>Two layer LSTM:</strong> two LSTM layers with 32 hidden units each followed by a dense head, dropout about 0.4, sequence length 1. Trained with weighted MSE, Adam at lr 5e-5 and weight decay 1e-3, early stopping on weighted R squared. Best LSTM score was about 0.004329, lagged responder variant slightly negative.</p>
              <p><strong>Ensembling:</strong> no blend deployed in the final submission; compared tree and RNN families to understand when trees dominate short-horizon market data. Future work notes include blending XGBoost and LSTM for regime diversification.</p>
              <p><strong>Link:</strong> <a href="https://github.com/AdomasFiseris/JaneStreetRTMDForecasting" target="_blank">GitHub repo</a> for the local modeling code and utilities.</p>
            </details>
          </div>
        </section>

        <h3 class="section-heading">Core stack</h3>
        <p>
          <span class="pill"><strong>Cloud:</strong> AWS (EC2 · RDS · S3 · SageMaker)</span>
          <span class="pill"><strong>Data:</strong> PostgreSQL</span>
          <span class="pill"><strong>Orchestration & CI:</strong> GitFlow · Jenkins</span>
          <span class="pill"><strong>Containers:</strong> Docker</span>
          <span class="pill"><strong>MLOps:</strong> MLflow</span>
          <span class="pill"><strong>Modeling:</strong> PyTorch · JAX · H2O.ai</span>
          <span class="pill"><strong>Language:</strong> Python</span>
        </p>

      </article>
    </main>

    <footer></footer>
  </div>

  <script src="assets/js/jquery.min.js"></script>
  <script src="assets/js/browser.min.js"></script>
  <script src="assets/js/breakpoints.min.js"></script>
  <script src="assets/js/util.js"></script>
  <script src="assets/js/main.js"></script>
</body>
</html>
